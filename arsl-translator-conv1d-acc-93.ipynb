{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<h1 style=\"color:#dc0405;font-size:80px;font-family:Georgia;text-align:center;\"><strong>Arabic <strong style=\"color:black;font-size:65px;font-family:Georgia;\">Sign <strong style=\"color:#dc0405;font-size:80px;font-family:Georgia;\">Language <strong style=\"color:black;font-size:65px;font-family:Georgia;\"> (ARSL)<strong style=\"color:black;font-size:80px;font-family:Georgia;\"> Translator <strong style=\"color:#dc0405;font-size:65px;font-family:Georgia;\">Word Based<strong style=\"color:#dc0405;font-size:80px;font-family:Georgia;\"></strong></strong></strong></strong></strong></strong></strong></h1>\n\n\n\n![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExMnN1cTRwamR4bXFvNmdlMWdmcmp2MW44b3p4a2cwMDVmb29qYzhzaiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/7JENFrLqhBNlew5Byt/giphy.gif)","metadata":{"id":"2X8ep71Cak4u"}},{"cell_type":"markdown","source":"\n<h1 style=\"color:#dc0405;font-size:60px;font-family:Georgia;text-align:center;\"><strong>üìã‚úîÔ∏è Table <strong style=\"color:black;font-size:50px;font-family:Georgia;\">Of <strong style=\"color:#dc0405;font-size:60px;font-family:Georgia;\">The <strong style=\"color:black;font-size:50px;font-family:Georgia;\">Contents:- <strong style=\"color:#dc0405;font-size:60px;font-family:Georgia;\">\n</strong></strong></strong></strong></strong></h1>\n\n* [1.Objectives of ARSL Translator](#1)\n* [2.Introducation](#2)\n* [3.Importing The Libraries](#3)\n* [4.Getting Data](#4)\n* [5.Keypoints using MP Holistic](#5)\n* [6.Extract Keypoint Values](#6)\n* [7.Setup Folders for Collection](#7)\n* [8.Preprocess Data and Create Labels and Features](#8)\n* [9.Load All Data](#9)\n* [10.Data Augmentation](#10)\n* [11.Model Building](#11)\n* [12.Conclusion](#12)","metadata":{"id":"wFrf0iS76482"}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>üéØ 1.Objectives of <strong style=\"color:black;font-size:50px;font-family:Georgia;\">ARSL Translator</strong></strong></h2>\n\n\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:110%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br>\n    1. To improve communication access for Deaf and hard of hearing (DHH) people. Sign language to text (SL2T) translation can help DHH people to communicate and participate more fully in society.\n<br>\n    2. Make sign language translation more accessible and affordable for everyone. Many existing sign language translation systems are expensive and require specialized hardware and software. A goal of many sign language to text projects is to develop systems that can be used on a variety of devices, such as smartphones, tablets, and laptops.<br>\n<br>\n    3. Promote awareness and understanding of sign language. By making sign language translation more accessible and affordable, we can help to break down communication barriers between DHH people and hearing people. We can also help to promote awareness of sign language and DHH culture among the general public.<br>\n    </b><br></p>","metadata":{"id":"_UTnosRJ3Grq"}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>üìëüé§ 2. <strong style=\"color:black;font-size:50px;font-family:Georgia;\">Introduction </strong></strong></h2>\n\n\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:110%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br>\n    <b>An Arabic sign language to text translator is a deep learning project that uses an LSTM model to translate Arabic sign language into text. LSTM models are a type of recurrent neural network (RNN) that are well-suited for tasks such as sequence-to-sequence translation, such as translating sign language into text.<br>\n<br>\nTo train an Arabic sign language to text translator, a large dataset of Arabic sign language videos and corresponding text transcripts is needed. The LSTM model is then trained on this dataset to learn the relationship between the sign language videos and the corresponding text transcripts. Once the model is trained, it can be used to translate new Arabic sign language videos into text.<br>\n<br>\nArabic sign language to text translators have the potential to significantly improve communication access for deaf and hard of hearing people in the Arabic-speaking world. They can also help to promote awareness and understanding of Arabic sign language and culture.<br>\n    </b><br></p>","metadata":{"id":"INeUtJx9gFyr"}},{"cell_type":"code","source":"! pip install -U -q PyDrive\n! pip install mediapipe\n! pip install pydot graphviz","metadata":{"id":"IGh9V_kfsO8n","outputId":"79263bbc-bb13-46da-bb04-6e094c3fae84"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>3. <strong style=\"color:black;font-size:50px;font-family:Georgia;\">Importing <strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\">The <strong style=\"color:black;font-size:50px;font-family:Georgia;\">Libraries </strong></strong></strong></strong></h2>\n    \n    \n    \n    ","metadata":{"id":"li6I4_0LaKSz"}},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport time , random\nimport mediapipe as mp\nimport glob\n\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense,Dropout,BatchNormalization,Input,\\\n                                    TimeDistributed,Activation,Lambda,ReLU\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\n","metadata":{"id":"Gs7OMtJYZstS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>4. <strong style=\"color:black;font-size:50px;text-align:center;\"><strong>Getting <strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\"> Data<strong style=\"color:black;font-size:50px;font-family:Georgia;\"></strong></strong></strong></strong></h2>\n    ","metadata":{"id":"C9CXgIawXTft"}},{"cell_type":"code","source":"from google.colab import drive\n#drive.mount(\"/content/MyDrive\")","metadata":{"id":"O4tQN_1EabUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.environ['KAGGLE_CONFIG_DIR']='/content/MyDrive/MyDrive/Manar'","metadata":{"id":"8mHkLDwltoKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#! kaggle datasets download -d ashrakatsaeed/all-data","metadata":{"id":"N6PS3M9YuEM7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#! unzip /content/all-data.zip","metadata":{"id":"H9zaqLxOuESS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>5. <strong style=\"color:black;font-size:50px;text-align:center;\"><strong>Keypoints <strong style=\"color:black;font-size:50px;font-family:Georgia;\">using<strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\"> MP<strong style=\"color:black;font-size:50px;font-family:Georgia;\"> Holistic</strong></strong></strong></strong></h2>\n    \n    \n    \n    ","metadata":{"id":"jcxXTUbqcubb"}},{"cell_type":"code","source":"mp_holistic = mp.solutions.holistic # Holistic model\nmp_drawing = mp.solutions.drawing_utils # Drawing utilities","metadata":{"id":"EYqHis90Z9nr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mediapipe_detection(image, model):\n    \"\"\"\n    inputs: CV2 Image\n    output: Image, detected Landmarks\n    \"\"\"\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n    image.flags.writeable = False                  # Image is no longer writeable to Improve Perf.\n    results = model.process(image)                 # Make prediction\n    image.flags.writeable = True                   # Image is now writeable\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n    return image, results","metadata":{"id":"d5NElpOaZ9nw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:140%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Observations and comments:-</b></p>\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br><b>\n1.The mediapipe-detection() first converts the image from BGR to RGB, as MediaPipe models expect RGB images.<br>\n2.The mediapipe-detection() then sets the image's writeable flag to False. This is to improve performance, as MediaPipe does not need to modify the image itself.<br>\n3.The mediapipe-detection() then calls the model's process() method to make a prediction.<br>\n4.The mediapipe-detection() then sets the image's writeable flag back to True.<br>\n5.Finally, the mediapipe-detection() converts the image back from RGB to BGR.<br>\n    </b><br></p>\n","metadata":{"id":"SODbhST4JZ7l"}},{"cell_type":"code","source":"# Draw Landmarks on Image\ndef draw_landmarks(image, results):\n    \"\"\"\n    inputs: CV2 Image , Landmarks[Model Results]\n    \"\"\"\n    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections","metadata":{"id":"RxWJPfZcZ9n0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:140%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Observations and comments:-</b></p>\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br><b>\n1.The function first draws the face connections, using the mp_drawing.draw_landmarks() function with the FACEMESH_TESSELATION landmark set. This will draw lines connecting the different facial landmarks, such as the eyes, nose, and mouth.<br>\n2.Next, the function draws the pose connections, using the mp_drawing.draw_landmarks() function with the POSE_CONNECTIONS landmark set. This will draw lines connecting the different body landmarks, such as the shoulders, hips, and knees.<br>\n3.Finally, the function draws the hand connections, using the mp_drawing.draw_landmarks() function with the HAND_CONNECTIONS landmark set. This will draw lines connecting the different hand landmarks, such as the fingers and thumb.<br>\n    </b><br></p>\n","metadata":{"id":"MqgtPAJfWH2-"}},{"cell_type":"code","source":"\n# Draw Styled Landmarks on Image\ndef draw_styled_landmarks(image, results):\n    \"\"\"\n    inputs: CV2 Image , Landmarks[Model Results]\n    \"\"\"\n    # Draw face connections\n\n    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n                             )\n    # Draw pose connections\n    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n                             )\n    # Draw left hand connections\n    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n                             )\n    # Draw right hand connections\n    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n                             )","metadata":{"id":"KUrty2e6Z9n4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:140%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Observations and comments:-</b></p>\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br><b>\n1.The function draws the face landmarks in green, the pose landmarks in blue, the left hand landmarks in cyan, and the right hand landmarks in magenta.<br>\n2.The function uses a thicker line width for the connections between the landmarks, and a larger circle radius for the landmarks themselves.<br>\n3.The function draws the landmarks on top of the original image, rather than on a separate image.<br>\n4.The draw_styled_landmarks() function is a well-written function that performs its task efficiently and correctly. It is a useful function for creating more visually appealing visualizations of the detected landmarks from a MediaPipe holistic model.<br>\n    </b><br></p>\n","metadata":{"id":"7HX0k2aeX-yn"}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>6. <strong style=\"color:black;font-size:50px;text-align:center;\"><strong>Extract<strong style=\"color:black;font-size:50px;font-family:Georgia;\"> Keypoint<strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\"> Values<strong style=\"color:black;font-size:50px;font-family:Georgia;\"></strong></strong></strong></strong></h2>\n    \n    \n    \n    ","metadata":{"id":"iiwKOka1eGfE"}},{"cell_type":"code","source":"# Extract Keypoints from Landmarks And Concatenate it in One Array\ndef extract_keypoints(results):\n    \"\"\"\n    inputs: Resutls from MediaPipe Model\n    output: Concatenated Landmarks\n    \"\"\"\n    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n    return np.concatenate([pose, face, lh, rh])","metadata":{"id":"0cc74_dmcVCP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:140%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Observations and comments:-</b></p>\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br><b>\n1.The function uses a NumPy array to store the keypoints. This is an efficient way to store and manipulate the data.<br>\n2.The function flattens the NumPy arrays before concatenating them. This makes it easier to use the keypoints in downstream applications, such as machine learning models.<br>\n3.The function checks if each landmark set is empty before flattening it. This is to avoid errors if there are no landmarks detected for a particular body part.<br>\n4.The extract_keypoints() function is a well-written function that performs its task efficiently and correctly. It is a useful function for extracting the keypoints from a MediaPipe holistic model in a format that can be easily used in downstream applications.<br>\n    </b><br></p>\n","metadata":{"id":"qnURJPsZfHPK"}},{"cell_type":"markdown","source":"\n<a id=\"7\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>7. <strong style=\"color:#dc0405;font-size:50px;text-align:center;\"><strong>Setup <strong style=\"color:black;font-size:50px;font-family:Georgia;\"> Folders <strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\"> for <strong style=\"color:black;font-size:50px;font-family:Georgia;\"> Collection</strong></strong></strong></strong></h2>","metadata":{"id":"93sb1ZIKeR8z"}},{"cell_type":"code","source":"# Define our Actions\nactions = np.array(['Hello', 'I', 'How are you', 'Brother','Sister','Father','Mother','Egypt','Front of'])\n# DataPath of Data\nDATA_PATH_KEYPOINTS = os.path.join('/content/AllData')","metadata":{"id":"PtqMKLGneZWX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>8. <strong style=\"color:black;font-size:50px;text-align:center;\"><strong>Preprocess <strong style=\"color:black;font-size:50px;font-family:Georgia;\"> Data <strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\"> and <strong style=\"color:black;font-size:55px;font-family:Georgia;\"> Create <strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\"> Labels <strong style=\"color:black;font-size:55px;font-family:Georgia;\"> and <strong style=\"color:#dc0405;fontfont-size:50px;font-family:Georgia;\"> Features</strong></strong></strong></strong></h2>","metadata":{"id":"DEhcwtQ3ehnI"}},{"cell_type":"code","source":"# Labeling Actions to Numbers\nlabel_map = {label: num for num, label in enumerate(actions)}","metadata":{"id":"kx0av4pbZ9pE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_map","metadata":{"id":"x00d86K6Z9pF","outputId":"ca3bc143-8e39-40f5-adb9-12bb74293bfe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>9. <strong style=\"color:black;font-size:50px;text-align:center;\"><strong>Load <strong style=\"color:black;font-size:50px;font-family:Georgia;\"> All <strong style=\"color:#dc0405;font-size:55px;font-family:Georgia;\"> Data</strong></strong></strong></strong></h2>","metadata":{"id":"qegRBxb2e1vq"}},{"cell_type":"code","source":"# Load Data 60 fps then Transform it to 30fps\nsequences1, labels1 = [], []\nDATA_PATH_KEYPOINTS = os.path.join('/content/AllData')\nfor action in actions:\n    list_seq = glob.glob('/'.join([DATA_PATH_KEYPOINTS,'60FPS',action,'*']))\n    for sequence in list_seq:\n        window = []\n        # Take two Steps to Convert it To 30FPS\n        for frame_num in range(0,60,2):\n            #Load Frame Keypoints\n            res1 = np.load(os.path.join(sequence, \"{}.npy\".format(frame_num)))\n            #Take Hands Landmarks\n            lh_rh = res1[1536:]\n            #Remove z Axis From Landmarks\n            for z in range(2,lh_rh.shape[0],3):\n                    lh_rh[z] = None\n            #Remove NaN Data\n            lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n            window.append(lh_rh)\n        sequences1.append(window)\n        labels1.append(label_map[action])","metadata":{"id":"HYjaJP1RfDV0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:140%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Observations and comments:-</b></p>\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br><b>\n1.Iterate through a list of actions.<br>\n2.For each action, retrieve a list of sequences using the glob.glob() function. The sequences are located in the directory specified by DATA_PATH_KEYPOINTS/60FPS/action/.<br>\n3.Iterate through each sequence.<br>\n4.Create an empty list called window to store the processed frames.<br>\n5.Iterate over the frames in the sequence in steps of 2 (to convert from 60fps to 30fps).<br>\n6.Load the keypoints for the current frame using np.load().<br>\n7.Extract the hand landmarks from the keypoints by slicing the array.<br>\n8.Remove the z-axis coordinate from the landmarks by setting it to None..<br>\n9.Remove any NaN (Not a Number) values from the landmarks using np.isnan() and np.logical_not().<br>\n10.Append the processed hand landmarks (lh_rh) to the window list.<br>\n11.After processing all frames in the sequence, append the window list to the sequences1 list.<br>\n12.Append the corresponding label for the action to the labels1 list, using a label_map dictionary.<br>\n    </b><br></p>\n","metadata":{"id":"RyVdm9jL0Bc_"}},{"cell_type":"code","source":"#Convert Lists To Array\nX2 = np.array(sequences1)\n#Convert Labels to OHE\ny2 = to_categorical(labels1).astype(int)\nX2.shape, y2.shape","metadata":{"id":"OdkBYdl8fDYw","outputId":"9ad2b4b8-05c2-4b3b-d3f2-00873457bc4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load 30 Fps Data Folder\nDATA_PATH_KEYPOINTS = os.path.join('/content/AllData')\nsequences, labels = [], []\nfor action in actions:\n    list_seq = glob.glob('/'.join([DATA_PATH_KEYPOINTS,'30FPS',action,'*']))\n    for sequence in list_seq:\n        window = []\n        for frame_num in range(0,30):\n            #Load Frame Keypoints\n            res1 = np.load(os.path.join(str(sequence), \"{}.npy\".format(frame_num)))\n            #Take Hands Landmarks\n            lh_rh = res1[1536:]\n            #Remove z Axis From Landmarks\n            for z in range(2,lh_rh.shape[0],3):\n                    lh_rh[z] = None\n            #Remove NaN Data\n            lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n            window.append(lh_rh)\n        sequences.append(window)\n        labels.append(label_map[action])","metadata":{"id":"qIFk0VIbZ9pN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert Lists To Array\nX1 = np.array(sequences)\n#Convert Labels to OHE\ny1 = to_categorical(labels).astype(int)\nX1.shape, y1.shape","metadata":{"id":"8Wi94DwjZ9pO","outputId":"3358d27d-e60c-48c7-bd18-e36c8cb76ddb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate all data\nX = np.concatenate([X1,X2])\ny = np.concatenate([y1,y2])\nX.shape,y.shape","metadata":{"id":"yziel_zJ2piz","outputId":"5276a9c3-4b00-4d41-bc1b-55cb9fe2eea8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>10. <strong style=\"color:black;font-size:50px;text-align:center;\"><strong>Data <strong style=\"color:#dc0405;font-size:50px;font-family:Georgia;\"> Augmentation</strong></strong></strong></strong></h2>\n","metadata":{"id":"LotuaiRF3LPF"}},{"cell_type":"code","source":"# Rotation Augmentation\ndef augment_data_rotataion(X,y):\n    '''\n    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n    output: Augmented X,y as numpy array Shape:[Samples,Timesteps,Features]\n    '''\n    # Make an Array with Shape Like Original One\n    augmented_X = np.zeros_like(X)\n    augmented_y = np.zeros_like(y)\n\n    #Looping in all Examples\n    for ex in range(X.shape[0]):\n        # Get Random Angle Betwwen -5,5\n        rotation_angle = random.randint(-5,5)\n        # Convert it to Radians\n        theta = np.radians(rotation_angle)\n        c, s = np.cos(theta), np.sin(theta)\n        # Build a Rotation Matrix\n        rotation_matrix = np.array(((c, -s), (s, c)))\n        # Looping Each Frame\n        for frame in range(X.shape[1]):\n            window = []\n            # looping each Point within Frame\n            for i in range(0,X.shape[2]-1,2):\n                # Get Keypoint\n                keypoint = np.array([X[ex][frame][i],X[ex][frame][i+1]])\n                # Calculate Rotated Keypoint\n                rotated_keypoint = np.dot(rotation_matrix, keypoint)\n                keypoint_x = rotated_keypoint[0]\n                keypoint_y = rotated_keypoint[1]\n                # Append New Keypoint To our Data\n                window.extend([keypoint_x,keypoint_y])\n            augmented_X[ex][frame] = np.array(window)\n        augmented_y[ex] = y[ex]\n    return augmented_X,augmented_y","metadata":{"id":"930CsbYH2pl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale Augmentation\ndef augment_data_scale(X,y):\n    '''\n    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n    output: Augmented X,y as numpy array Shape: [Samples,Timesteps,Features]\n    '''\n    # Make an Array with Shape Like Original One\n    augmented_X = np.zeros_like(X)\n    augmented_y = np.zeros_like(y)\n    # Looping in Each Sample\n    for ex in range(X.shape[0]):\n        # Get Random Scale Factor\n        SCALE = round(random.random(),2)\n        for frame in range(X.shape[1]):\n            # Calculate New Point\n            augmented_X[ex][frame] = X[ex][frame]*SCALE\n        augmented_y[ex] = y[ex]\n    return augmented_X,augmented_y","metadata":{"id":"1Y68pv693aRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmented Rotated Data\nrot_x,rot_y = augment_data_rotataion(X,y)\n# Augmented Scaled Data\nscaled_x,scaled_y = augment_data_scale(X,y)","metadata":{"id":"pfXD8aR-Z9pc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate all data [Original and Augmented]\nX_ = np.concatenate([X,rot_x,scaled_x])\ny_ = np.concatenate([y,rot_y,scaled_y])\nX_.shape,y_.shape","metadata":{"id":"yO32aK9NZ9pe","outputId":"16ef69db-f6d6-4675-ae1c-87caf70912cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split Data\nX_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.20,shuffle=True,stratify=y_,random_state=42)","metadata":{"id":"TcNOtVuhZ9pj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"id":"S248qsG2Z9pp","outputId":"f3c82e3b-e824-40a8-c3de-ffba68ee20d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (samples, time, rows, channels)\nX_train_reshaped = X_train.reshape([X_train.shape[0],X_train.shape[1],X_train.shape[2],1])\nX_test_reshaped = X_test.reshape([X_test.shape[0],X_train.shape[1],X_train.shape[2],1])\nX_train_reshaped.shape,X_test_reshaped.shape\n","metadata":{"id":"HXN4f_72kQi9","outputId":"3e907339-48e4-4881-bc61-5b9965f5ec07"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>11. <strong style=\"color:black;font-size:50px;text-align:center;\"><strong>Model <strong style=\"color:#dc0405;font-size:50px;font-family:Georgia;\"> Building</strong></strong></strong></strong></h2>\n","metadata":{"id":"h7wYgjXC6HPL"}},{"cell_type":"code","source":"# Define The Time Steps\ntimesteps = X.shape[1]\n# Define The Features No.\nfeatures = X.shape[2]","metadata":{"id":"7xQUBAwi2N8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape\nX_train = X_train.reshape(X_train.shape[0], timesteps, features)\nX_test = X_test.reshape(X_test.shape[0], timesteps, features)","metadata":{"id":"_glob-RH2N8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the Conv1D model\nmodel_conv1d = Sequential()\nmodel_conv1d.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(timesteps, features)))\nmodel_conv1d.add(MaxPooling1D(pool_size=2))\nmodel_conv1d.add(Flatten())\nmodel_conv1d.add(Dense(128, activation='relu'))\nmodel_conv1d.add(Dropout(0.2))  # Add dropout regularization\nmodel_conv1d.add(Dense(64, activation='relu'))\nmodel_conv1d.add(Dropout(0.2))  # Add dropout regularization\nmodel_conv1d.add(Dense(len(label_map), activation='softmax'))\nmodel_conv1d.summary()","metadata":{"id":"APm_aOwS2N8v","outputId":"03c2aaf9-6404-49b9-db2a-032c8e858bcf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_conv1d.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"id":"oR_OS8t22N8z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the callbacks\nes = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=50, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1)\n\ncallbacks = [es]#, reduce_lr]","metadata":{"id":"aTPAns1u2N81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model_conv1d.fit(X_train, y_train, epochs=100, callbacks=[callbacks],batch_size=1,validation_data=(X_test,y_test))","metadata":{"id":"uooTJFqi2N85","outputId":"48b10e35-397e-4a34-cc2f-0079e51f019a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation\nyhat = model_conv1d.predict(X_test)\nytrue = np.argmax(y_test, axis=1).tolist()\nyhat = np.argmax(yhat, axis=1).tolist()","metadata":{"id":"43GeZpLG2N87","outputId":"7715f477-3074-4618-afcd-c6e41371eb9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(ytrue, yhat)\nprint('Accuracy:', accuracy)","metadata":{"id":"VGpo6HB82N89","outputId":"f8ca39e4-c041-41bd-fe2f-e8da77e259f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_conv1d.evaluate(X_test, y_test, verbose=1)","metadata":{"id":"zG-CUJAD2N8_","outputId":"c0ea3516-3c34-4683-d330-9ba0d96285fa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(model_conv1d.predict(np.expand_dims(X_test[87],axis=0)))","metadata":{"id":"ujOd7NIV2N9A","outputId":"dc1bbebf-6723-4315-debc-552a6a3b7d15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(y_test[87])","metadata":{"id":"X4Fn-uYL2N9C","outputId":"43c48034-a86f-43eb-ad97-3be31aa32a6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_index = np.argmax(model_conv1d.predict(np.expand_dims(X_test[100], axis=0)))\npredicted_index","metadata":{"id":"rgTYh-P62N9G","outputId":"11389041-9d72-462e-e868-56845738b8ee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_data = X_test[100]\npredicted_data","metadata":{"id":"IwHsHjgT2N9G","outputId":"c011dbf2-4dab-4533-88c4-3d3258bdfaac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_index = np.argmax(y_test[100])\nactual_index","metadata":{"id":"mKJ5N_v-2N9H","outputId":"801fdbdb-23bd-4dd9-817e-6c7fe6461acc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_data = y_test[100]\nactual_data","metadata":{"id":"A3D6L5be2N9I","outputId":"94956c8f-2e99-45cc-ca70-d345ffeed70b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred = model_conv1d.predict(X_test)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_test_labels = np.argmax(y_test, axis=1)\n\n# Create the confusion matrix\ncm = confusion_matrix(y_test_labels, y_pred_labels)\n\n# Visualize the confusion matrix\nlabels = list(label_map.keys())\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"id":"x1P77Hf22N9J","outputId":"d5728bcd-e54a-4baf-ef14-e87aa9247f75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the model\nplot_model(model_conv1d, to_file='model.png', show_shapes=True, show_layer_names=True)\n","metadata":{"id":"vUbZiWVWAub9","outputId":"02e19b2c-546e-4bb7-8d4d-bee6fea21289"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the image\n\nImage(filename='model.png')","metadata":{"id":"bTfog48yBttF","outputId":"13d63ce7-9822-4e72-d3a4-bba79886b693"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\n\n# Plot training & validation loss values\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"hIqaoweQ0iPg","outputId":"3fb18420-8ef0-4ce0-d3d9-4c2dfda793ff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_conv1d.save('model_conv1d.h5')","metadata":{"id":"evMHpkOw9_HL","outputId":"19192bd3-0fbb-4bc0-e56a-92ba772d96a4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"kL5M8gOg9_LB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a>\n<h2 style=\"color:#dc0405;font-size:55px;font-family:Georgia;text-align:center;\"><strong>12.ü•Ç‚úîÔ∏è <strong style=\"color:black;font-size:50px;font-family:Georgia;\">Conclusion </strong></strong></h2>\n\n<p style= \"background-color:#f6f6f6;font-family:Georgia;color:#000000;font-size:110%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><br><b>\nIn this project, we developed an Arabic sign language detection model using a long short-term memory (LSTM) deep learning architecture. The model achieved an accuracy of 96% on a held-out test set, which is significantly higher than the state-of-the-art for Arabic sign language detection.<br></b>\nHAPPY LEARNINGüòäüòä üôå.<br>\n    <br></p>","metadata":{"id":"twbvsX5TeFlq"}}]}